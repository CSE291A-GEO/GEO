{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88aa26c4",
   "metadata": {},
   "source": [
    "# MACO-Style Multi-Agent Content Optimization (Paper-Aligned)\n",
    "\n",
    "This notebook implements the full pipeline, including frozen corpus, evaluator with MIS/ISR/MIV, iterative optimization loop, analyst/editor agents, and hybrid selector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517388b",
   "metadata": {},
   "source": [
    "## 0) Setup & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c09c5cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca718104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, hashlib, re, textwrap\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# NOTE: API keys and secrets are loaded from environment variables (see .env or your shell config)\n",
    "# Example expected variables (DO NOT hard-code real values here):\n",
    "#   GOOGLE_API_KEY=\"***\"\n",
    "#   LANGSMITH_API_KEY=\"***\"\n",
    "#   LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "#   GENSEE_API_KEY=\"***\"\n",
    "\n",
    "# Model choices & constants\n",
    "MODEL_EVAL     = \"gemini-2.5-flash\"\n",
    "MODEL_ANALYST  = \"gemini-2.5-flash\"\n",
    "MODEL_EDITOR   = \"gemini-2.5-flash-lite\"\n",
    "TEMPERATURE_EVAL    = 0.0\n",
    "TEMPERATURE_ANALYST = 0.6\n",
    "TEMPERATURE_EDITOR  = 0.1\n",
    "\n",
    "N_QUERIES   = 3        # 5–10, the paper uses 10\n",
    "MAX_CTX     = 3       # contexts per query, the paper uses 10\n",
    "SUCCESS_TAU = 0.75     # ISR threshold\n",
    "N_ITERS     = 5       # iterations; selector often picks ~, the paper uses 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# TODO: update the anchors, the paper uses [0,10]\n",
    "ANCHORS = [0.00, 0.17, 0.33, 0.50, 0.67, 0.83, 1.00]\n",
    "METRICS = [\"CP\",\"AA\",\"FA\",\"KC\",\"SC\",\"AD\"]\n",
    "\n",
    "# TODO: baseline-style labeling\n",
    "# Optional: tag detection for edits (baseline-style labeling)\n",
    "TAG_PATTERNS = [\n",
    "    (\"Statistics\",    r\"\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\s?%|\\b(?:million|billion|thousand)\\b\"),\n",
    "    (\"More Quotes\",   r\"[\\\"“][^\\\"”]{8,}[\\\"”]\"),\n",
    "    (\"Citing Sources\",r\"\\b(?:According to|Source:|cited by|as reported by)\\b\"),\n",
    "    (\"Technical Terms\", r\"\\b(latency|throughput|gradient|API|OAuth|schema|vector|embedding|protocol|REST|GraphQL)\\b\"),\n",
    "    (\"Authoritative\", r\"\\b(must|should|undoubtedly|certainly|we recommend)\\b\"),\n",
    "    (\"Fluent\",        r\".\"),  # fallback: any edit without the above\n",
    "]\n",
    "\n",
    "# Reproducibility tweaks where applicable\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf8e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "def log_heading(h: str):\n",
    "    \"\"\"Log a heading - both prints to console and writes to log file\"\"\"\n",
    "    if DEBUG:\n",
    "        print(\"\\n\" + \"=\"*8 + \" \" + h + \" \" + \"=\"*8)\n",
    "\n",
    "def log_json(name: str, obj):\n",
    "    \"\"\"Log JSON object - both prints to console and writes to log file\"\"\"\n",
    "    if DEBUG:\n",
    "        print(f\"\\n[{name}]\")\n",
    "        try:\n",
    "            print(json.dumps(obj, ensure_ascii=False, indent=2))\n",
    "        except Exception:\n",
    "            print(str(obj)[:2000])\n",
    "\n",
    "def log_info(message: str):\n",
    "    \"\"\"Helper function to log info messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "    print(f\"[{timestamp}] {message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f29fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOGGING SETUP =====\n",
    "class TeeOutput:\n",
    "    \"\"\"Class to capture stdout/stderr and write to both console and file\"\"\"\n",
    "    def __init__(self, terminal, log_file):\n",
    "        self.terminal = terminal\n",
    "        self.log_file = log_file\n",
    "        self.file_handle = None\n",
    "        self._open_file()\n",
    "        \n",
    "    def _open_file(self):\n",
    "        \"\"\"Open file handle for writing\"\"\"\n",
    "        self.file_handle = open(self.log_file, 'w', encoding='utf-8')\n",
    "        \n",
    "    def write(self, message):\n",
    "        # Write to terminal\n",
    "        self.terminal.write(message)\n",
    "        # Write to file (only if message is not empty)\n",
    "        if message and self.file_handle:\n",
    "            self.file_handle.write(message)\n",
    "            self.file_handle.flush()\n",
    "        \n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        if self.file_handle:\n",
    "            self.file_handle.flush()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close file handle\"\"\"\n",
    "        if self.file_handle:\n",
    "            self.file_handle.close()\n",
    "            self.file_handle = None\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging to timestamped file. Returns log file path.\"\"\"\n",
    "    now = datetime.now()\n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_dir = \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filename: YYYY_MM_DD_HH_MM.txt (e.g., 2025_01_15_14_30.txt)\n",
    "    log_filename = f\"{now.year:04d}_{now.month:02d}_{now.day:02d}_{now.hour:02d}_{now.minute:02d}.txt\"\n",
    "    log_path = os.path.join(log_dir, log_filename)\n",
    "    \n",
    "    # Store original stdout/stderr\n",
    "    original_stdout = sys.stdout\n",
    "    original_stderr = sys.stderr\n",
    "    \n",
    "    # Create TeeOutput instances (they will open the file)\n",
    "    tee_stdout = TeeOutput(original_stdout, log_path)\n",
    "    tee_stderr = TeeOutput(original_stderr, log_path)\n",
    "    \n",
    "    # Redirect stdout and stderr to TeeOutput\n",
    "    sys.stdout = tee_stdout\n",
    "    sys.stderr = tee_stderr\n",
    "    \n",
    "    # Write header (this will go through TeeOutput, so no duplication)\n",
    "    print('='*80)\n",
    "    print(f\"MACO Pipeline Log - Started at {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Log file: {log_path}\")\n",
    "    print('='*80 + '\\n')\n",
    "    \n",
    "    return log_path\n",
    "\n",
    "# Initialize logging\n",
    "LOG_FILE_PATH = setup_logging()\n",
    "print(f\"[LOG] All output will be saved to: {LOG_FILE_PATH}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3932b5c",
   "metadata": {},
   "source": [
    "## 1) LLM client (LangChain Google GenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2906ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def make_llm(model: str, temperature: float):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_retries=0,\n",
    "        max_output_tokens=8192,\n",
    "        # relies on GOOGLE_API_KEY env var\n",
    "    )\n",
    "\n",
    "def call_llm_json(llm, system: str, user: str, retry: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call an LLM with system+user text and parse JSON output robustly.\n",
    "    If schema fails, return {\"__SCHEMA_ERROR__\": raw_text}\n",
    "    \"\"\"\n",
    "    msgs = [(\"system\", system), (\"human\", user)]\n",
    "    out = llm.invoke(msgs)\n",
    "    \n",
    "    # --- FIX START: Handle list content from Gemini ---\n",
    "    val = getattr(out, \"content\", \"\")\n",
    "    if isinstance(val, list):\n",
    "        # If content is a list, join it into a single string\n",
    "        text = \"\".join(str(v) for v in val)\n",
    "    else:\n",
    "        text = str(val) if val else str(out)\n",
    "    # --- FIX END ---\n",
    "    \n",
    "    # Strip fencing if present\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", text, flags=re.S)\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        if retry > 0:\n",
    "            nudged = textwrap.dedent(f\"\"\"Your previous reply was not valid JSON. Reprint ONLY strict JSON, no commentary. Original reply: {text}\"\"\")\n",
    "            out2 = llm.invoke([(\"system\", system), (\"human\", nudged)])\n",
    "            \n",
    "            # --- FIX START: Handle list content for retry as well ---\n",
    "            val2 = getattr(out2, \"content\", \"\")\n",
    "            if isinstance(val2, list):\n",
    "                t2 = \"\".join(str(v) for v in val2)\n",
    "            else:\n",
    "                t2 = str(val2) if val2 else str(out2)\n",
    "            # --- FIX END ---\n",
    "            \n",
    "            t2 = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", t2.strip(), flags=re.S)\n",
    "            try:\n",
    "                return json.loads(t2)\n",
    "            except Exception:\n",
    "                return {\"__SCHEMA_ERROR__\": t2}\n",
    "        return {\"__SCHEMA_ERROR__\": text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49366ba",
   "metadata": {},
   "source": [
    "## 2) Retrieval (Gensee AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00b68547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "def gensee_ai_retrieve(query: str, max_results: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves context snippets using the Gensee AI Platform API.\n",
    "\n",
    "    Notes:\n",
    "        - Relies on an environment variable `GENSEE_API_KEY` to get the 'Bearer your_token_here'.\n",
    "        - Returns an empty list if the request fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    api_key = os.getenv(\"GENSEE_API_KEY\",)\n",
    "    if not api_key:\n",
    "        print(\"[WARN] Missing GENSEE_API_KEY environment variable — returning empty list.\")\n",
    "        return []\n",
    "\n",
    "    # 2. Prepare the API request\n",
    "    url = 'https://platform.gensee.ai/tool/search'\n",
    "    \n",
    "    # 3. Build the payload matching your API's requirements\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'max_results': max_results\n",
    "    }\n",
    "    \n",
    "    # 4. Build the headers matching your API's requirements\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}' # Dynamically load the key from env\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 5. Send the POST request\n",
    "        response = requests.post(url, json=data, headers=headers, timeout=60)\n",
    "        response.raise_for_status() # Raise an exception for bad statuses (401, 403, 500, etc.)\n",
    "        data = response.json()\n",
    "\n",
    "        # 6. Parse your specific JSON response structure\n",
    "        #    Based on your example, results are in the 'search_response' key\n",
    "        results = data.get(\"search_response\", [])\n",
    "        \n",
    "        contexts = []\n",
    "        for item in results:\n",
    "            # Based on your example, the text snippet is in the 'content' key\n",
    "            snippet = item.get(\"content\") or \"\"\n",
    "            if snippet:\n",
    "                contexts.append(snippet)\n",
    "        \n",
    "        # 7. Ensure a List[str] is returned\n",
    "        return [ctx for ctx in contexts if ctx][:max_results]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[WARN] Gensee AI request failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f2540",
   "metadata": {},
   "source": [
    "## 3) Prompts (Query, Evaluator, Analyst, Editor, Selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b2aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUERY_SYSTEM = \"\"\"You generate user queries for evaluating ONE article.\n",
    "Produce content-centric queries:\n",
    "- Each query MUST be answerable using the article alone.\n",
    "- Cover varied user intents: {definition, learn/explanation, how-to/guide, compare, list/examples}.\n",
    "- Avoid brand bait, clickbait, or unanswerable questions.\n",
    "\n",
    "CRITICAL FORMATTING RULES:\n",
    "1. Return ONLY the raw JSON object.\n",
    "2. DO NOT use Markdown fencing (no ```json ... ```).\n",
    "3. DO NOT add any conversational text (e.g., \"Here are the queries...\").\n",
    "\n",
    "Example Output:\n",
    "{\"queries\":[\n",
    "  {\"intent\":\"definition\",\"q\":\"...\"},\n",
    "  {\"intent\":\"learn\",\"q\":\"...\"},\n",
    "  {\"intent\":\"how-to\",\"q\":\"...\"},\n",
    "  {\"intent\":\"compare\",\"q\":\"...\"},\n",
    "  {\"intent\":\"list\",\"q\":\"...\"}\n",
    "]}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_query_user(doc: str, n_queries: int = N_QUERIES) -> str:\n",
    "    return f\"[ARTICLE]\\n{doc}\\n\\nReturn {n_queries} queries spread across the intents.\"\n",
    "\n",
    "PROMPT_EVAL_SYSTEM = \"\"\"You are an evaluation judge. Given a candidate article and external contexts:\n",
    "1) Answer the user query (RAG style).\n",
    "2) Score the CANDIDATE ARTICLE on SIX dimensions in [0,1] using anchors {0,.17,.33,.5,.67,.83,1}:\n",
    "   - CP (Citation Prominence): clear, prominent citation/attribution of the candidate article in the final answer.\n",
    "   - AA (Attribution Accuracy): statements attributed to the article truly originate from it.\n",
    "   - FA (Faithfulness): answer remains faithful to the article’s meaning (no distortions).\n",
    "   - KC (Key Concepts): article covers essential concepts needed for this query.\n",
    "   - SC (Semantic Contribution): article contributes unique/central meaning vs other contexts.\n",
    "   - AD (Answer Dominance): overall share of answer content deriving from the article vs other contexts.\n",
    "Rules:\n",
    "- Judge ONLY the candidate article’s contribution; do not reward contexts.\n",
    "- If the answer can be formed without the article, penalize SC and AD.\n",
    "- If external contexts are absent or minimal relative to the answer, DO NOT award SC or AD above 0.33 unless you explicitly justify why the article itself supplies the necessary unique content.\n",
    "- If the article is very short/sparse and lacks definitions/examples/comparisons needed by the query, reduce KC and FA accordingly.\n",
    "\n",
    "CRITICAL FORMATTING RULES:\n",
    "1. Return ONLY the raw JSON object.\n",
    "2. DO NOT use Markdown fencing (no ```json ... ```).\n",
    "3. DO NOT include any preamble or postscript.\n",
    "\n",
    "Example Output:\n",
    "{\n",
    " \"answer\": \"...\",\n",
    " \"scores\": {\"CP\":0.83,\"AA\":0.67,\"FA\":0.83,\"KC\":0.67,\"SC\":0.50,\"AD\":0.50},\n",
    " \"why\": {\n",
    "   \"CP\":\"...\", \"AA\":\"...\", \"FA\":\"...\", \"KC\":\"...\", \"SC\":\"...\", \"AD\":\"...\"\n",
    " }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_eval_user(query: str, doc: str, contexts: List[str]) -> str:\n",
    "    ctx = \"\\n---\\n\".join(contexts[:MAX_CTX]) if contexts else \"(no external contexts)\"\n",
    "    return f\"[QUERY]\\n{query}\\n\\n[CANDIDATE_ARTICLE]\\n{doc}\\n\\n[CONTEXTS]\\n{ctx}\"\n",
    "\n",
    "PROMPT_ANALYST_SYSTEM = \"\"\"You propose targeted edits to improve the article’s weakest metrics.\n",
    "Inputs: (1) article, (2) per-query scores with brief rationales, (3) aggregate MIS/ISR/MIV.\n",
    "Find the single weakest metric by MIS; break ties by high MIV and low ISR.\n",
    "Propose up to 3 precise edits. For EACH edit include:\n",
    "- target_metric: one of {CP,AA,FA,KC,SC,AD}\n",
    "- reason: ≤2 sentences\n",
    "- location_hint: exact anchor text or section title\n",
    "- operation: one of {\"insert_after\",\"replace_span\",\"append_section\",\"delete_span\",\"merge_sections\"}\n",
    "- patch: exact text to insert/replace (≤180 words)\n",
    "\n",
    "CRITICAL FORMATTING RULES:\n",
    "1. Return ONLY the raw JSON object.\n",
    "2. DO NOT use Markdown fencing (no ```json ... ```).\n",
    "3. DO NOT add conversational filler.\n",
    "\n",
    "Example Output:\n",
    "{\"edits\":[{\"target_metric\":\"CP\",\"reason\":\"...\",\"location_hint\":\"...\",\"operation\":\"insert_after\",\"patch\":\"...\"}]}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_analyst_user(doc: str, per_query: List[Dict[str, Any]], agg: Dict[str, Any]) -> str:\n",
    "    return json.dumps({\n",
    "        \"article\": doc,\n",
    "        \"per_query\": per_query,\n",
    "        \"aggregate\": agg\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "PROMPT_EDITOR_SYSTEM = \"\"\"Apply ONE provided edit to the article faithfully. \n",
    "Do NOT rewrite unrelated text. If location_hint not found, place patch in the nearest logical spot.\n",
    "Return the FULL revised article only. No explanations.\n",
    "\"\"\"\n",
    "\n",
    "def prompt_editor_user(doc: str, json_edit: Dict[str, Any]) -> str:\n",
    "    return json.dumps({\"article\": doc, \"edit\": json_edit}, ensure_ascii=False)\n",
    "\n",
    "PROMPT_SELECTOR_SYSTEM = \"\"\"You are a selector comparing multiple article versions evaluated on the SAME query+context corpus.\n",
    "Given MIS, ISR, MIV per version, pick the version that maximizes:\n",
    "score = sum(MIS[m] for m in [CP,AA,FA,KC,SC,AD]) - 0.2 * sum(MIV[m] for m in [CP,AA,FA,KC,SC,AD]).\n",
    "Return your entire response in STRICT JSON:: {\"winner_index\": k, \"reason\":\"≤2 sentences\"}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_selector_user(history_summary: List[Dict[str, Any]]) -> str:\n",
    "    # history_summary: [{\"idx\": i, \"agg\": {...}, \"snippet\": \"...\"}]\n",
    "    return json.dumps({\"candidates\": history_summary}, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adf5e2",
   "metadata": {},
   "source": [
    "## 4) Query generation + frozen corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff1994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus (build once, then freeze) \n",
    "def generate_queries_from_doc(doc_text: str, n_queries: int = N_QUERIES) -> List[str]:\n",
    "    llm = make_llm(MODEL_ANALYST, temperature=0.3)  # tiny diversity, still on-topic\n",
    "    payload = call_llm_json(llm, PROMPT_QUERY_SYSTEM, prompt_query_user(doc_text, n_queries))\n",
    "    if \"__SCHEMA_ERROR__\" in payload:\n",
    "        # very robust fallback: produce 5 generic but doc-specific queries\n",
    "        base = [\n",
    "            \"Give a concise definition.\",\n",
    "            \"Explain the key benefits.\",\n",
    "            \"Provide a simple example.\",\n",
    "            \"Compare it with an alternative.\",\n",
    "            \"Give a short step-by-step guide.\"\n",
    "        ]\n",
    "        return [f\"{q} (based on the article above)\" for q in base][:n_queries]\n",
    "    qs = [q[\"q\"] for q in payload.get(\"queries\", []) if q.get(\"q\")]\n",
    "    # dedupe, cap\n",
    "    seen, uniq = set(), []\n",
    "    for q in qs:\n",
    "        if q not in seen:\n",
    "            uniq.append(q)\n",
    "            seen.add(q)\n",
    "    \n",
    "    if DEBUG:\n",
    "        log_heading(\"Query Agent: generated queries\")\n",
    "        for i, q in enumerate(uniq[:n_queries]):\n",
    "            print(f\"{i+1}. {q}\")\n",
    "\n",
    "    return uniq[:n_queries]\n",
    "\n",
    "def build_corpus_for_doc(doc_text: str, retriever=gensee_ai_retrieve,\n",
    "                         n_queries=N_QUERIES, max_ctx=MAX_CTX) -> Dict[str, Any]:\n",
    "    queries = generate_queries_from_doc(doc_text, n_queries=n_queries)\n",
    "    pairs = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            ctxs = retriever(q)[:max_ctx]\n",
    "        except Exception as e:\n",
    "            ctxs = []\n",
    "        # keep only queries with at least 2 contexts (so the judge can compare)\n",
    "        cleaned = []\n",
    "        for c in ctxs:\n",
    "            c = re.sub(r\"\\s+\", \" \", c.strip())\n",
    "            if c and c not in cleaned:\n",
    "                cleaned.append(c)\n",
    "        if len(cleaned) >= 2:\n",
    "            pairs.append({\"q\": q, \"ctx\": cleaned})\n",
    "    if DEBUG:\n",
    "        log_heading(\"Retrieval: per-query context counts\")\n",
    "        for p in pairs:\n",
    "            print(f\"- {p['q'][:80]}...  | ctx={len(p['ctx'])}\")\n",
    "        log_heading(\"Retrieved Contexts (Full Content)\")\n",
    "        for i, p in enumerate(pairs):\n",
    "            print(f\"\\n--- Query {i+1}: {p['q']} ---\")\n",
    "            for j, ctx in enumerate(p['ctx']):\n",
    "                print(f\"\\n[Context {j+1}]\")\n",
    "                print(ctx[:500] + (\"...\" if len(ctx) > 500 else \"\"))\n",
    "\n",
    "    # require minimum coverage\n",
    "    if len(pairs) < 2:\n",
    "        raise RuntimeError(f\"Corpus too small ({len(pairs)} with >=2 contexts). \"\n",
    "                           f\"Set GENSEE_API_KEY and retry, or reduce filters.\")\n",
    "    key = hashlib.md5(doc_text.encode()).hexdigest()[:10]\n",
    "    path = f\"corpus_{key}.json\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"queries\": pairs, \"created_at\": time.time()}, f, ensure_ascii=False, indent=2)\n",
    "    return {\"queries\": pairs, \"path\": path}\n",
    "\n",
    "    \n",
    "def load_corpus(path: str) -> Dict[str, Any]:\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a6b0c",
   "metadata": {},
   "source": [
    "## 5) Evaluator (per-query + MIS/ISR/MIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fbdc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def _nearest_anchor(x: float) -> float:\n",
    "    # snap to anchor grid\n",
    "    if x is None: return 0.0\n",
    "    try: x = float(x)\n",
    "    except: return 0.0\n",
    "    return min(ANCHORS, key=lambda a: abs(a - x))\n",
    "\n",
    "def evaluator_score(document: str, query: str, contexts: List[str]) -> Dict[str, Any]:\n",
    "    if DEBUG:\n",
    "        log_heading(f\"Evaluator: Query & Contexts\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"\\nNumber of contexts: {len(contexts)}\")\n",
    "        for i, ctx in enumerate(contexts):\n",
    "            print(f\"\\n[Context {i+1}]\")\n",
    "            print(ctx[:500] + (\"...\" if len(ctx) > 500 else \"\"))\n",
    "    \n",
    "    llm = make_llm(MODEL_EVAL, TEMPERATURE_EVAL)\n",
    "    payload = call_llm_json(llm, PROMPT_EVAL_SYSTEM, prompt_eval_user(query, document, contexts))\n",
    "    if DEBUG:\n",
    "        log_heading(\"Evaluator Output\")\n",
    "        log_json(\"payload\", payload)\n",
    "\n",
    "    answer = payload.get(\"answer\", \"\")\n",
    "    raw_scores = (payload.get(\"scores\") or {})\n",
    "    why = payload.get(\"why\") or {}\n",
    "    # coerce to anchors & fill missing\n",
    "    scores = {m: _nearest_anchor(raw_scores.get(m)) for m in METRICS}\n",
    "    return {\"query\": query, \"scores\": scores, \"why\": why, \"answer\": answer}\n",
    "\n",
    "def aggregate_scores(per_query_scores: List[Dict[str, Any]], tau: float = SUCCESS_TAU) -> Dict[str, Dict[str, float]]:\n",
    "    arr = np.array([[pq[\"scores\"][m] for m in METRICS] for pq in per_query_scores])  # shape Qx6\n",
    "    mis = dict(zip(METRICS, arr.mean(axis=0).round(4).tolist()))\n",
    "    isr = dict(zip(METRICS, (arr >= tau).mean(axis=0).round(4).tolist()))\n",
    "    miv = dict(zip(METRICS, arr.var(axis=0, ddof=0).round(4).tolist()))\n",
    "    return {\"MIS\": mis, \"ISR\": isr, \"MIV\": miv}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a8cd9",
   "metadata": {},
   "source": [
    "## 6) Analyst (edits) + tag detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70980ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst_propose_edits(doc: str, per_query: List[Dict[str, Any]], agg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    llm = make_llm(MODEL_ANALYST, TEMPERATURE_ANALYST)\n",
    "    payload = call_llm_json(llm, PROMPT_ANALYST_SYSTEM, prompt_analyst_user(doc, per_query, agg))\n",
    "    if DEBUG:\n",
    "        log_heading(\"Analyst: proposed edits\")\n",
    "        log_json(\"edits\", payload)\n",
    "\n",
    "    if \"__SCHEMA_ERROR__\" in payload:\n",
    "        # conservative fallback: add benefits sentence (improves SC/KC)\n",
    "        return {\"edits\": [{\n",
    "            \"target_metric\": \"SC\",\n",
    "            \"reason\": \"Add explicit benefits to improve semantic contribution and sufficiency.\",\n",
    "            \"location_hint\": \"After introduction\",\n",
    "            \"operation\": \"insert_after\",\n",
    "            \"patch\": \"Key benefits include clarity, coverage of essential concepts, and concrete examples that distinguish this article from generic sources.\"\n",
    "        }]}\n",
    "    # auto-tag the proposed patches\n",
    "    for e in payload.get(\"edits\", []):\n",
    "        patch = e.get(\"patch\", \"\")\n",
    "        for tag, pat in TAG_PATTERNS:\n",
    "            if re.search(pat, patch, flags=re.I):\n",
    "                e[\"tag\"] = tag\n",
    "                break\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf84a7",
   "metadata": {},
   "source": [
    "## 7) Editor (apply one edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "019c39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_edit_locally(doc: str, edit: Dict[str, Any]) -> str:\n",
    "    \"\"\"Lightweight, deterministic local editor for simple ops before LLM.\"\"\"\n",
    "    op = edit.get(\"operation\")\n",
    "    hint = edit.get(\"location_hint\",\"\")\n",
    "    patch = edit.get(\"patch\",\"\").strip()\n",
    "\n",
    "    if not patch and op != \"delete_span\":\n",
    "        return doc\n",
    "\n",
    "    if op == \"insert_after\":\n",
    "        idx = doc.find(hint) if hint else -1\n",
    "        if idx >= 0:\n",
    "            cut = idx + len(hint)\n",
    "            return doc[:cut] + (\"\\n\" if doc[cut:cut+1] != \"\\n\" else \"\") + patch + \"\\n\" + doc[cut:]\n",
    "        else:\n",
    "            # append near end\n",
    "            return doc.rstrip() + \"\\n\\n\" + patch + \"\\n\"\n",
    "\n",
    "    if op == \"replace_span\":\n",
    "        if hint and hint in doc:\n",
    "            return doc.replace(hint, patch, 1)\n",
    "        return doc  # fallback: no-op\n",
    "\n",
    "    if op == \"append_section\":\n",
    "        return doc.rstrip() + \"\\n\\n\" + patch + \"\\n\"\n",
    "\n",
    "    if op == \"delete_span\":\n",
    "        if hint and hint in doc:\n",
    "            return doc.replace(hint, \"\", 1)\n",
    "        return doc\n",
    "\n",
    "    if op == \"merge_sections\":\n",
    "        # naive: remove duplicate consecutive blank lines (simplify structure)\n",
    "        merged = re.sub(r\"\\n{3,}\", \"\\n\\n\", doc)\n",
    "        return merged\n",
    "\n",
    "    return doc\n",
    "\n",
    "def editor_apply_edit(doc: str, chosen_edit: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    First try a deterministic local application; if the hint isn't found or\n",
    "    the operation needs rewriting, fall back to the LLM editor.\n",
    "    \"\"\"\n",
    "    # Try local\n",
    "    new_doc = _apply_edit_locally(doc, chosen_edit)\n",
    "    if new_doc != doc or chosen_edit.get(\"operation\") in (\"append_section\",\"merge_sections\",\"delete_span\"):\n",
    "        return new_doc\n",
    "\n",
    "    # Fallback to LLM editor for tougher cases\n",
    "    llm = make_llm(MODEL_EDITOR, TEMPERATURE_EDITOR)\n",
    "    out = llm.invoke([(\"system\", PROMPT_EDITOR_SYSTEM),\n",
    "                      (\"human\", prompt_editor_user(doc, chosen_edit))])\n",
    "    text = getattr(out, \"content\", \"\") or str(out)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817b66e",
   "metadata": {},
   "source": [
    "## 8) Optimize loop + hybrid selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bc4d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _history_summary_for_selector(history: List[Tuple[str, List[Dict[str,Any]], Dict[str,Any]]]) -> List[Dict[str, Any]]:\n",
    "    summ = []\n",
    "    for i, (doc, perq, agg) in enumerate(history):\n",
    "        # a short snippet for context\n",
    "        snippet = (doc[:220] + \"…\") if len(doc) > 220 else doc\n",
    "        summ.append({\"idx\": i, \"agg\": agg, \"snippet\": snippet})\n",
    "    return summ\n",
    "\n",
    "def score_scalar(agg: Dict[str, Dict[str, float]], lam: float = 0.2) -> float:\n",
    "    s = sum(agg[\"MIS\"][m] for m in METRICS) - lam * sum(agg[\"MIV\"][m] for m in METRICS)\n",
    "    return round(float(s), 4)\n",
    "\n",
    "def select_best_version(history: List[Tuple[str, List[Dict[str,Any]], Dict[str,Any]]]) -> Dict[str, Any]:\n",
    "    # 1) rule-based ranking\n",
    "    with_scores = [(i, score_scalar(agg)) for i, (_, _, agg) in enumerate(history)]\n",
    "    with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top = [i for i,_ in with_scores[:3]]\n",
    "\n",
    "    # 2) LLM selector tie-breaker among top-3 (optional; safer)\n",
    "    llm = make_llm(MODEL_EVAL, 0.0)\n",
    "    summary = _history_summary_for_selector([history[i] for i in top])\n",
    "    payload = call_llm_json(llm, PROMPT_SELECTOR_SYSTEM, prompt_selector_user(summary))\n",
    "    if \"__SCHEMA_ERROR__\" in payload:\n",
    "        # fallback to best scalar\n",
    "        best_idx = top[0]\n",
    "    else:\n",
    "        k = payload.get(\"winner_index\", 0)\n",
    "        best_idx = top[min(max(int(k), 0), len(top)-1)]\n",
    "\n",
    "    doc, perq, agg = history[best_idx]\n",
    "    return {\"index\": best_idx, \"doc\": doc, \"agg\": agg, \"score_scalar\": score_scalar(agg)}\n",
    "\n",
    "def optimize_doc(doc_text: str, corpus: Dict[str, Any], n_iters: int = N_ITERS):\n",
    "    history = []\n",
    "    D = doc_text\n",
    "    for t in range(n_iters):\n",
    "        # Evaluate on the frozen corpus\n",
    "        per_query_scores = []\n",
    "        for item in corpus[\"queries\"]:\n",
    "            scores = evaluator_score(D, item[\"q\"], item[\"ctx\"])\n",
    "            per_query_scores.append(scores)\n",
    "        agg = aggregate_scores(per_query_scores, tau=SUCCESS_TAU)\n",
    "        history.append((D, per_query_scores, agg))\n",
    "\n",
    "        # Analyze & choose an edit\n",
    "        plan = analyst_propose_edits(D, per_query_scores, agg)\n",
    "        edits = plan.get(\"edits\", [])\n",
    "        if not edits:\n",
    "            # nothing to do -> early stop\n",
    "            break\n",
    "        # Choose the edit most aligned with weakest metric (by MIS)\n",
    "        mis = agg[\"MIS\"]\n",
    "        weakest = sorted(METRICS, key=lambda m: mis[m])[0]\n",
    "        chosen = next((e for e in edits if e.get(\"target_metric\")==weakest), edits[0])\n",
    "        if DEBUG:\n",
    "            log_heading(f\"ITER {t} — Chosen edit\")\n",
    "            log_json(\"chosen_edit\", chosen)\n",
    "\n",
    "        # Apply\n",
    "        D = editor_apply_edit(D, chosen)\n",
    "\n",
    "        if DEBUG:\n",
    "            log_heading(f\"ITER {t} — Editor Output (New Document)\")\n",
    "            print(D) \n",
    "            print(\"=\"*80)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485022a",
   "metadata": {},
   "source": [
    "## 9) Data Generation Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c2fdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def load_articles_from_tsv(tsv_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load articles from TSV file.\n",
    "    Expected format: query, source_url, se_rank, ge_rank, clean_content (tab-separated).\n",
    "    Maps: query -> article_id, clean_content -> article_text\n",
    "    Returns list of dicts with 'article_id' and 'article_text'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # Check required columns\n",
    "    if 'query' not in df.columns or 'clean_content' not in df.columns:\n",
    "        raise ValueError(f\"TSV must have 'query' and 'clean_content' columns. Found: {df.columns.tolist()}\")\n",
    "    \n",
    "    articles = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Skip rows with empty or very short content\n",
    "        content = str(row['clean_content']).strip()\n",
    "        if len(content) < 50:\n",
    "            continue\n",
    "            \n",
    "        articles.append({\n",
    "            'article_id': str(row['query']),  # Use query as article_id\n",
    "            'article_text': content            # Use clean_content as article_text\n",
    "        })\n",
    "    \n",
    "    log_info(f\"Loaded {len(articles)} valid articles from {tsv_path}\")\n",
    "    return articles\n",
    "\n",
    "def run_single_article_experiment(article_id: str, article_text: str, n_iters: int = 5, n_generated_queries: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run optimization experiment for a single article.\n",
    "    \n",
    "    Args:\n",
    "        article_id: Unique identifier for the article\n",
    "        article_text: The target article text to optimize\n",
    "        n_iters: Number of optimization iterations\n",
    "        n_generated_queries: Number of queries to generate from target article (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - 'article_id': str\n",
    "            - 'original_article': str\n",
    "            - 'best_edited_article': str\n",
    "            - 'best_iteration': int\n",
    "            - 'iteration_metrics': List[Dict[str, float]] (6 metrics per iteration)\n",
    "            - 'generated_queries': List[str] (queries generated from target article)\n",
    "            - 'success': bool\n",
    "    \"\"\"\n",
    "    log_heading(f\"Processing Article: {article_id}\")\n",
    "    \n",
    "    # Validate input\n",
    "    if not article_text or len(article_text.strip()) < 50:\n",
    "        return {'article_id': article_id, 'success': False, 'error': 'Article text too short or empty'}\n",
    "    \n",
    "    target_article = article_text.strip()\n",
    "    \n",
    "    # Log the original article\n",
    "    log_heading(\"Original Target Article\")\n",
    "    print(f\"Article ID: {article_id}\")\n",
    "    print(f\"Article length: {len(target_article)} chars\")\n",
    "    print(f\"Article preview (first 500 chars):\\n{target_article[:500]}...\\n\")\n",
    "    \n",
    "    log_info(f\"Target article length: {len(target_article)} chars\")\n",
    "    \n",
    "    # Step 1: Generate queries from target article\n",
    "    log_heading(\"Generating Queries from Target Article\")\n",
    "    try:\n",
    "        generated_queries = generate_queries_from_doc(target_article, n_queries=n_generated_queries)\n",
    "        log_info(f\"✓ Generated {len(generated_queries)} queries:\")\n",
    "        for i, gq in enumerate(generated_queries):\n",
    "            print(f\"  {i+1}. {gq[:100]}{'...' if len(gq) > 100 else ''}\")\n",
    "    except Exception as e:\n",
    "        log_info(f\"✗ Failed to generate queries: {e}\")\n",
    "        return {'article_id': article_id, 'success': False, 'error': f'Query generation failed: {e}'}\n",
    "    \n",
    "    if len(generated_queries) < 2:\n",
    "        return {'article_id': article_id, 'success': False, 'error': f'Too few queries generated: {len(generated_queries)}'}\n",
    "    \n",
    "    # Step 2: Retrieve contexts for each generated query\n",
    "    log_heading(\"Retrieving Contexts for Generated Queries\")\n",
    "    corpus_queries = []\n",
    "    \n",
    "    for i, gq in enumerate(generated_queries):\n",
    "        log_info(f\"Retrieving contexts for generated query {i+1}/{len(generated_queries)}\")\n",
    "        try:\n",
    "            # Retrieve 4 contexts per generated query\n",
    "            gq_contexts = gensee_ai_retrieve(gq, max_results=4)\n",
    "            \n",
    "            # Basic cleaning\n",
    "            cleaned_ctxs = []\n",
    "            seen_c = set()\n",
    "            for c in gq_contexts:\n",
    "                if isinstance(c, str):\n",
    "                    c_clean = c.strip()\n",
    "                    if c_clean and c_clean not in seen_c:\n",
    "                        cleaned_ctxs.append(c_clean)\n",
    "                        seen_c.add(c_clean)\n",
    "            \n",
    "            log_info(f\"  -> Retrieved {len(cleaned_ctxs)} contexts\")\n",
    "            corpus_queries.append({'q': gq, 'ctx': cleaned_ctxs})\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_info(f\"  -> ✗ Error retrieving contexts: {e}\")\n",
    "            corpus_queries.append({'q': gq, 'ctx': []})\n",
    "            \n",
    "    corpus = {\n",
    "        'queries': corpus_queries\n",
    "    }\n",
    "    \n",
    "    log_info(f\"Corpus created with {len(corpus['queries'])} query-context pairs\")\n",
    "    \n",
    "    # Step 3: Run optimization iterations\n",
    "    history = []\n",
    "    D = target_article\n",
    "    \n",
    "    for t in range(n_iters):\n",
    "        log_heading(f\"Iteration {t+1}/{n_iters}\")\n",
    "        \n",
    "        # Evaluate current document\n",
    "        log_heading(f\"Evaluating Article - Iteration {t}\")\n",
    "        print(f\"Article length: {len(D)} chars\")\n",
    "        print(f\"Article preview (first 300 chars):\\n{D[:300]}...\\n\")\n",
    "        \n",
    "        # Evaluate on each generated query\n",
    "        per_query_scores = []\n",
    "        for i, item in enumerate(corpus[\"queries\"]):\n",
    "            scores = evaluator_score(D, item[\"q\"], item[\"ctx\"])\n",
    "            per_query_scores.append(scores)\n",
    "            # Log individual query metrics\n",
    "            query_metrics = \" \".join([f\"{m}:{scores['scores'][m]:.2f}\" for m in METRICS])\n",
    "            log_info(f\"  Query {i+1}/{len(corpus['queries'])} | {query_metrics}\")\n",
    "        \n",
    "        # Aggregate metrics across all generated queries\n",
    "        agg = aggregate_scores(per_query_scores, tau=SUCCESS_TAU)\n",
    "        history.append((D, per_query_scores, agg))\n",
    "        \n",
    "        # Log aggregated metrics for this iteration\n",
    "        log_heading(f\"Iteration {t} - Aggregated Metrics\")\n",
    "        mis_line = \" \".join([f\"{m}:{agg['MIS'][m]:.2f}\" for m in METRICS])\n",
    "        isr_line = \" \".join([f\"{m}:{agg['ISR'][m]:.2f}\" for m in METRICS])\n",
    "        miv_line = \" \".join([f\"{m}:{agg['MIV'][m]:.4f}\" for m in METRICS])\n",
    "        log_info(f\"MIS | {mis_line}\")\n",
    "        log_info(f\"ISR | {isr_line}\")\n",
    "        log_info(f\"MIV | {miv_line}\")\n",
    "        \n",
    "        # Analyze and apply edit (skip on last iteration)\n",
    "        if t < n_iters - 1:\n",
    "            plan = analyst_propose_edits(D, per_query_scores, agg)\n",
    "            edits = plan.get(\"edits\", [])\n",
    "            \n",
    "            if not edits:\n",
    "                log_info(\"No edits proposed, stopping early\")\n",
    "                break\n",
    "            \n",
    "            # Choose edit targeting weakest metric\n",
    "            mis = agg[\"MIS\"]\n",
    "            weakest = sorted(METRICS, key=lambda m: mis[m])[0]\n",
    "            chosen = next((e for e in edits if e.get(\"target_metric\")==weakest), edits[0])\n",
    "            \n",
    "            log_info(f\"Applying edit targeting: {chosen.get('target_metric')}\")\n",
    "            \n",
    "            # Apply edit\n",
    "            D = editor_apply_edit(D, chosen)\n",
    "            \n",
    "            # Log the edited article\n",
    "            log_heading(f\"Edited Article - After Iteration {t}\")\n",
    "            print(f\"New length: {len(D)} chars\")\n",
    "            print(f\"Preview (first 300 chars):\\n{D[:300]}...\\n\")\n",
    "    \n",
    "    # Step 4: Select best version\n",
    "    best = select_best_version(history)\n",
    "    \n",
    "    # Log the best selected article\n",
    "    log_heading(\"Best Article Selected\")\n",
    "    print(f\"Best iteration: {best['index']}\")\n",
    "    print(f\"Best score: {best['score_scalar']:.3f}\")\n",
    "    print(f\"Best article length: {len(best['doc'])} chars\")\n",
    "    print(f\"Best article preview (first 500 chars):\\n{best['doc'][:500]}...\\n\")\n",
    "    \n",
    "    # Step 5: Extract metrics for each iteration\n",
    "    iteration_metrics = []\n",
    "    for i, (_, _, agg) in enumerate(history):\n",
    "        metrics_dict = {m: agg['MIS'][m] for m in METRICS}\n",
    "        iteration_metrics.append(metrics_dict)\n",
    "    \n",
    "    log_info(f\"Best iteration: {best['index']} with score: {best['score_scalar']:.3f}\")\n",
    "    log_info(f\"Generated {len(generated_queries)} queries for evaluation\")\n",
    "    \n",
    "    return {\n",
    "        'article_id': article_id,\n",
    "        'original_article': target_article,\n",
    "        'best_edited_article': best['doc'],\n",
    "        'best_iteration': best['index'],\n",
    "        'iteration_metrics': iteration_metrics,\n",
    "        'generated_queries': generated_queries,\n",
    "        'success': True\n",
    "    }\n",
    "\n",
    "def save_experiment_results(results: List[Dict[str, Any]], output_dir: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Save experiment results to two CSV files.\n",
    "    \n",
    "    File 1: articles.csv - contains article_id, original_article, best_edited_article\n",
    "    File 2: metrics.csv - contains article_id and metrics for each iteration\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # File 1: Articles\n",
    "    articles_file = os.path.join(output_dir, 'articles.csv')\n",
    "    articles_exists = os.path.exists(articles_file)\n",
    "    with open(articles_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write header only if file is new\n",
    "        if not articles_exists:\n",
    "            writer.writerow(['article_id', 'original_article', 'best_edited_article', 'best_iteration'])\n",
    "        \n",
    "        for r in results:\n",
    "            if r.get('success'):\n",
    "                writer.writerow([\n",
    "                    r['article_id'],\n",
    "                    r['original_article'],\n",
    "                    r['best_edited_article'],\n",
    "                    r['best_iteration']\n",
    "                ])\n",
    "    \n",
    "    log_info(f\"{'Appended to' if articles_exists else 'Created'} articles file: {articles_file}\")\n",
    "    \n",
    "    # File 2: Metrics\n",
    "    metrics_file = os.path.join(output_dir, 'metrics.csv')\n",
    "    metrics_exists = os.path.exists(metrics_file)\n",
    "    with open(metrics_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        # Create header: article_id, iter0_CP, iter0_AA, ..., iter4_AD\n",
    "        header = ['article_id']\n",
    "\n",
    "        successes = [r for r in results if r.get(\"success\")]\n",
    "        if not successes:\n",
    "            log_info(\"No successful results, skipping CSV write\")\n",
    "            return None, None\n",
    "        max_iters = max(len(r.get(\"iteration_metrics\", [])) for r in successes)\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            for metric in METRICS:\n",
    "                header.append(f'iter{i}_{metric}')\n",
    "        \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write header only if file is new\n",
    "        if not metrics_exists:\n",
    "            writer.writerow(header)\n",
    "        \n",
    "        for r in results:\n",
    "            if r.get('success'):\n",
    "                row = [r['article_id']]\n",
    "                for iter_metrics in r['iteration_metrics']:\n",
    "                    for metric in METRICS:\n",
    "                        row.append(iter_metrics.get(metric, ''))\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    log_info(f\"{'Appended to' if metrics_exists else 'Created'} metrics file: {metrics_file}\")\n",
    "    \n",
    "    return articles_file, metrics_file\n",
    "\n",
    "def run_full_experiment(tsv_path: str, n_articles: int = None, n_iters: int = 5):\n",
    "    \"\"\"\n",
    "    Run the full data generation experiment.\n",
    "    \n",
    "    Args:\n",
    "        tsv_path: Path to TSV file containing articles\n",
    "        n_articles: Number of articles to process (None = all)\n",
    "        n_iters: Number of optimization iterations per article\n",
    "    \"\"\"\n",
    "    log_heading(\"Starting Data Generation Experiment\")\n",
    "    \n",
    "    # Load articles\n",
    "    all_articles = load_articles_from_tsv(tsv_path)\n",
    "    \n",
    "    if n_articles:\n",
    "        articles_to_process = all_articles[:n_articles]\n",
    "    else:\n",
    "        articles_to_process = all_articles\n",
    "    \n",
    "    log_info(f\"Processing {len(articles_to_process)} articles with {n_iters} iterations each\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    for i, article in enumerate(articles_to_process):\n",
    "        log_heading(f\"Article {i+1}/{len(articles_to_process)}\")\n",
    "        try:\n",
    "            result = run_single_article_experiment(\n",
    "                article['article_id'], \n",
    "                article['article_text'], \n",
    "                n_iters=n_iters\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            if result.get('success'):\n",
    "                log_info(f\"✓ Successfully processed article {i+1}\")\n",
    "            else:\n",
    "                log_info(f\"✗ Failed to process article {i+1}: {result.get('error')}\")\n",
    "        except Exception as e:\n",
    "            log_info(f\"✗ Exception processing article {i+1}: {e}\")\n",
    "            results.append({'article_id': article['article_id'], 'success': False, 'error': str(e)})\n",
    "    \n",
    "    # Save results\n",
    "    articles_file, metrics_file = save_experiment_results(results)\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r.get('success'))\n",
    "    log_heading(\"Experiment Complete\")\n",
    "    log_info(f\"Successfully processed: {successful}/{len(results)} articles\")\n",
    "    log_info(f\"Results saved to:\")\n",
    "    log_info(f\"  - Articles: {articles_file}\")\n",
    "    log_info(f\"  - Metrics: {metrics_file}\")\n",
    "    \n",
    "    return results, articles_file, metrics_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment_run",
   "metadata": {},
   "source": [
    "## 10) Run Experiment\n",
    "\n",
    "Uncomment and run the cell below to start the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "run_experiment",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 240\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, articles_file, metrics_file\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# ==================== RUN EXPERIMENT ====================\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m results, articles_file, metrics_file \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 173\u001b[0m, in \u001b[0;36mrun_experiment_with_config\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 Article ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Preview: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreview\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Run single query experiment\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_article_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_ITERATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_generated_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_GENERATED_QUERIES\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[27], line 90\u001b[0m, in \u001b[0;36mrun_single_article_experiment\u001b[0;34m(article_id, article_text, n_iters, n_generated_queries)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving contexts for generated query \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(generated_queries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Retrieve 4 contexts per generated query\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     gq_contexts \u001b[38;5;241m=\u001b[39m \u001b[43mgensee_ai_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Basic cleaning\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     cleaned_ctxs \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m, in \u001b[0;36mgensee_ai_retrieve\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m     30\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Dynamically load the key from env\u001b[39;00m\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# 5. Send the POST request\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status() \u001b[38;5;66;03m# Raise an exception for bad statuses (401, 403, 500, etc.)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/GEO/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==================== EXPERIMENT CONFIGURATION ====================\n",
    "\n",
    "# Input/Output Settings \n",
    "TSV_PATH = \"../data/ariticles/target_articles.tsv\"  # Path to target_articles CSV\n",
    "OUTPUT_DIR = \"data\"                                      # Output directory\n",
    "\n",
    "# Experiment Settings \n",
    "N_ARTICLES_TO_PROCESS = 100    # Number of articles to process (set to None for all)\n",
    "                             \n",
    "START_INDEX = 0              # Start from which article (useful for batching)\n",
    "                             \n",
    "N_ITERATIONS = 5             # Number of optimization iterations per article\n",
    "\n",
    "N_GENERATED_QUERIES = 5      # Number of queries to generate from target article (3-10 recommended)\n",
    "\n",
    "# Resume Settings - NEW!\n",
    "RESUME_FROM_EXISTING = True   # Skip articles that are already in metrics file\n",
    "EXISTING_METRICS_FILE = None  # Specify file path, or None to auto-detect latest\n",
    "                             \n",
    "\n",
    "# Retrieval Settings\n",
    "# N_CONTEXTS_TO_RETRIEVE removed (not needed anymore)   # Number of contexts to retrieve per article\n",
    "MIN_CONTEXTS_REQUIRED = 2    # Minimum contexts needed to process article\n",
    "\n",
    "# Model Settings (optional, using defaults from config)\n",
    "MODEL_EVAL = \"gemini-2.5-flash\"\n",
    "MODEL_ANALYST = \"gemini-2.5-flash\"\n",
    "MODEL_EDITOR = \"gemini-2.5-flash\"  # Fixed typo: flah -> flash\n",
    "\n",
    "# Debug Settings \n",
    "VERBOSE = True              # Print detailed progress \n",
    "SAVE_INTERMEDIATE = True    # Save after each article (slower but safer)\n",
    "\n",
    "# ==================================================================\n",
    "\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def find_latest_metrics_file(output_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if metrics.csv exists and has data.\n",
    "    Returns filepath if exists with data, None otherwise.\n",
    "    \"\"\"\n",
    "    metrics_file = os.path.join(output_dir, \"metrics.csv\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(metrics_file):\n",
    "        return None\n",
    "    \n",
    "    # Check if file has actual data (not just header)\n",
    "    try:\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        if len(df) > 0:  # Has at least 1 data row\n",
    "            return metrics_file\n",
    "        else:  # Only header, treat as non-existent\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_processed_articles(metrics_file: str) -> set:\n",
    "    \"\"\"Load queries that have already been processed\"\"\"\n",
    "    if not metrics_file or not os.path.exists(metrics_file):\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        processed = set(df['article_id'].tolist())\n",
    "        log_info(f\"📂 Loaded {len(processed)} processed articles from: {metrics_file}\")\n",
    "        return processed\n",
    "    except Exception as e:\n",
    "        log_info(f\"⚠️  Could not load existing metrics file: {e}\")\n",
    "        return set()\n",
    "\n",
    "def filter_unprocessed_articles(all_articles: list, processed_articles: set) -> list:\n",
    "    \"\"\"Filter out articles that have already been processed\"\"\"\n",
    "    unprocessed = [a for a in all_articles if a[\"article_id\"] not in processed_articles]\n",
    "    skipped = len(all_articles) - len(unprocessed)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        log_info(f\"⏭️  Skipping {skipped} already processed queries\")\n",
    "        log_info(f\"📋 {len(unprocessed)} queries remaining to process\")\n",
    "    \n",
    "    return unprocessed\n",
    "\n",
    "\n",
    "# ==================== RUN EXPERIMENT ====================\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def run_experiment_with_config():\n",
    "    \"\"\"Run experiment with above configuration\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"🚀 Starting Experiment at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n📋 Configuration:\")\n",
    "    print(f\"  - TSV Path: {TSV_PATH}\")\n",
    "    print(f\"  - Articles to process: {N_ARTICLES_TO_PROCESS if N_ARTICLES_TO_PROCESS else 'ALL'}\")\n",
    "    print(f\"  - Start index: {START_INDEX}\")\n",
    "    print(f\"  - Iterations per article: {N_ITERATIONS}\")\n",
    "    print(f\"  - Generated queries per article: {N_GENERATED_QUERIES}\")\n",
    "    print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"  - Resume from existing: {RESUME_FROM_EXISTING}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ✅ FIX 1: Define output file paths at START\n",
    "    articles_file = os.path.join(OUTPUT_DIR, 'articles.csv')\n",
    "    metrics_file = os.path.join(OUTPUT_DIR, 'metrics.csv')\n",
    "    \n",
    "    # Load queries\n",
    "    all_articles = load_articles_from_tsv(TSV_PATH)\n",
    "    log_info(f\"📊 Total articles in TSV: {len(all_articles)}\")\n",
    "    \n",
    "    # Check for existing processed queries (RESUME FEATURE)\n",
    "    processed_articles = set()\n",
    "    if RESUME_FROM_EXISTING:\n",
    "        # Find existing metrics file\n",
    "        metrics_file_to_check = EXISTING_METRICS_FILE\n",
    "        if not metrics_file_to_check:\n",
    "            metrics_file_to_check = find_latest_metrics_file(OUTPUT_DIR)\n",
    "        \n",
    "        if metrics_file_to_check:\n",
    "            log_info(f\"🔍 Checking for processed queries in: {metrics_file_to_check}\")\n",
    "            processed_articles = load_processed_articles(metrics_file_to_check)\n",
    "        else:\n",
    "            log_info(f\"ℹ️  No existing metrics file found, starting fresh\")\n",
    "    \n",
    "    # Select queries based on configuration\n",
    "    # Filter out already processed queries FIRST\n",
    "    all_unprocessed = filter_unprocessed_articles(all_articles, processed_articles)\n",
    "    \n",
    "    # Then select based on START_INDEX and N_ARTICLES_TO_PROCESS\n",
    "    if N_ARTICLES_TO_PROCESS:\n",
    "        end_index = min(START_INDEX + N_ARTICLES_TO_PROCESS, len(all_unprocessed))\n",
    "        articles_to_process = all_unprocessed[START_INDEX:end_index]\n",
    "    else:\n",
    "        articles_to_process = all_unprocessed[START_INDEX:]\n",
    "    \n",
    "    \n",
    "    if len(articles_to_process) == 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ All queries already processed! Nothing to do.\")\n",
    "        print(\"=\"*80)\n",
    "        return [], articles_file, metrics_file\n",
    "    \n",
    "    log_info(f\"📊 Processing queries: {len(articles_to_process)} remaining\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, article in enumerate(articles_to_process):\n",
    "        # Find actual index in original query list\n",
    "        actual_idx = next((idx for idx, a in enumerate(all_articles) if a[\"article_id\"] == article[\"article_id\"]), i)\n",
    "        \n",
    "        log_heading(f\"Query {actual_idx + 1}/{len(all_articles)} (Processing {i+1}/{len(articles_to_process)})\")\n",
    "        \n",
    "        if VERBOSE:\n",
    "            article_id = article[\"article_id\"]\n",
    "            article_text = article[\"article_text\"]\n",
    "            preview = article_text[:80]\n",
    "            suffix = '...' if len(article_text) > 80 else ''\n",
    "            print(f\"📝 Article ID: {article_id} | Preview: {preview}{suffix}\")\n",
    "        \n",
    "        try:\n",
    "            # Run single query experiment\n",
    "            result = run_single_article_experiment(\n",
    "                article[\"article_id\"],\n",
    "                article[\"article_text\"], \n",
    "                n_iters=N_ITERATIONS,\n",
    "                n_generated_queries=N_GENERATED_QUERIES\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            if result.get('success'):\n",
    "                successful += 1\n",
    "                log_info(f\"✅ Success | Total: {successful}/{i+1}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                log_info(f\"❌ Failed: {result.get('error')} | Total failures: {failed}/{i+1}\")\n",
    "            \n",
    "            # ✅ FIX 2: Save only the NEW result (not accumulated list)\n",
    "            if results:\n",
    "                log_info(f\"💾 Saving intermediate results...\")\n",
    "                save_experiment_results([result], output_dir=OUTPUT_DIR)\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            log_info(f\"❌ Exception: {str(e)[:100]}\")\n",
    "            results.append({\n",
    "                'article_id': article[\"article_id\"], \n",
    "                'success': False, \n",
    "                'error': str(e)\n",
    "            })\n",
    "        \n",
    "        # Progress summary every 10 queries\n",
    "        if (i + 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / (i + 1)\n",
    "            remaining = avg_time * (len(articles_to_process) - i - 1)\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"📊 Progress: {i+1}/{len(articles_to_process)} articles processed\")\n",
    "            print(f\"✅ Successful: {successful} | ❌ Failed: {failed}\")\n",
    "            print(f\"⏱️  Elapsed: {elapsed/60:.1f} min | Estimated remaining: {remaining/60:.1f} min\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Note: Results are already saved after each query\n",
    "    # No need to save again at the end\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 EXPERIMENT COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  - Total processed this run: {len(results)}\")\n",
    "    print(f\"  - Successful: {successful} ({successful/len(results)*100:.1f}%)\")\n",
    "    print(f\"  - Failed: {failed} ({failed/len(results)*100:.1f}%)\")\n",
    "    print(f\"  - Skipped (already done): {len(processed_articles)}\")\n",
    "    print(f\"  - Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"  - Average per article: {total_time/len(results):.1f} seconds\")\n",
    "    \n",
    "    print(f\"\\n📁 Output Files:\")\n",
    "    print(f\"  - Articles: {articles_file}\")\n",
    "    print(f\"  - Metrics:  {metrics_file}\")\n",
    "    \n",
    "    # ✅ FIX 3: Add return statement\n",
    "    return results, articles_file, metrics_file\n",
    "\n",
    "\n",
    "# ==================== RUN EXPERIMENT ====================\n",
    "\n",
    "# Run the experiment\n",
    "results, articles_file, metrics_file = run_experiment_with_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa8474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
