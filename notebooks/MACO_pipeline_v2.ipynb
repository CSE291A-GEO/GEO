{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88aa26c4",
   "metadata": {},
   "source": [
    "# MACO-Style Multi-Agent Content Optimization (Paper-Aligned)\n",
    "\n",
    "This notebook implements the full pipeline, including frozen corpus, evaluator with MIS/ISR/MIV, iterative optimization loop, analyst/editor agents, and hybrid selector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517388b",
   "metadata": {},
   "source": [
    "## 0) Setup & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c09c5cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca718104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, hashlib, re, textwrap\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# NOTE: API keys and secrets are loaded from environment variables (see .env or your shell config)\n",
    "# Example expected variables (DO NOT hard-code real values here):\n",
    "#   GOOGLE_API_KEY=\"***\"\n",
    "#   LANGSMITH_API_KEY=\"***\"\n",
    "#   LANGSMITH_TRACING=\"true\"      # optional\n",
    "#   LANGSMITH_PROJECT=\"***\"\n",
    "#   LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "#   GENSEE_API_KEY=\"***\"\n",
    "\n",
    "# Model choices & constants\n",
    "MODEL_EVAL     = \"gemini-2.5-flash\"\n",
    "MODEL_ANALYST  = \"gemini-2.5-flash\"\n",
    "MODEL_EDITOR   = \"gemini-2.5-flash-lite\"\n",
    "TEMPERATURE_EVAL    = 0.0\n",
    "TEMPERATURE_ANALYST = 0.6\n",
    "TEMPERATURE_EDITOR  = 0.1\n",
    "\n",
    "N_QUERIES   = 3        # 5–10, the paper uses 10\n",
    "MAX_CTX     = 3       # contexts per query, the paper uses 10\n",
    "SUCCESS_TAU = 0.75     # ISR threshold\n",
    "N_ITERS     = 5       # iterations; selector often picks ~, the paper uses 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# TODO: update the anchors, the paper uses [0,10]\n",
    "ANCHORS = [0.00, 0.17, 0.33, 0.50, 0.67, 0.83, 1.00]\n",
    "METRICS = [\"CP\",\"AA\",\"FA\",\"KC\",\"SC\",\"AD\"]\n",
    "\n",
    "# TODO: baseline-style labeling\n",
    "# Optional: tag detection for edits (baseline-style labeling)\n",
    "TAG_PATTERNS = [\n",
    "    (\"Statistics\",    r\"\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\s?%|\\b(?:million|billion|thousand)\\b\"),\n",
    "    (\"More Quotes\",   r\"[\\\"“][^\\\"”]{8,}[\\\"”]\"),\n",
    "    (\"Citing Sources\",r\"\\b(?:According to|Source:|cited by|as reported by)\\b\"),\n",
    "    (\"Technical Terms\", r\"\\b(latency|throughput|gradient|API|OAuth|schema|vector|embedding|protocol|REST|GraphQL)\\b\"),\n",
    "    (\"Authoritative\", r\"\\b(must|should|undoubtedly|certainly|we recommend)\\b\"),\n",
    "    (\"Fluent\",        r\".\"),  # fallback: any edit without the above\n",
    "]\n",
    "\n",
    "# Reproducibility tweaks where applicable\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daf8e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "def log_heading(h: str):\n",
    "    \"\"\"Log a heading - both prints to console and writes to log file\"\"\"\n",
    "    if DEBUG:\n",
    "        print(\"\\n\" + \"=\"*8 + \" \" + h + \" \" + \"=\"*8)\n",
    "\n",
    "def log_json(name: str, obj):\n",
    "    \"\"\"Log JSON object - both prints to console and writes to log file\"\"\"\n",
    "    if DEBUG:\n",
    "        print(f\"\\n[{name}]\")\n",
    "        try:\n",
    "            print(json.dumps(obj, ensure_ascii=False, indent=2))\n",
    "        except Exception:\n",
    "            print(str(obj)[:2000])\n",
    "\n",
    "def log_info(message: str):\n",
    "    \"\"\"Helper function to log info messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "    print(f\"[{timestamp}] {message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f29fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOGGING SETUP =====\n",
    "class TeeOutput:\n",
    "    \"\"\"Class to capture stdout/stderr and write to both console and file\"\"\"\n",
    "    def __init__(self, terminal, log_file):\n",
    "        self.terminal = terminal\n",
    "        self.log_file = log_file\n",
    "        self.file_handle = None\n",
    "        self._open_file()\n",
    "        \n",
    "    def _open_file(self):\n",
    "        \"\"\"Open file handle for writing\"\"\"\n",
    "        self.file_handle = open(self.log_file, 'w', encoding='utf-8')\n",
    "        \n",
    "    def write(self, message):\n",
    "        # Write to terminal\n",
    "        self.terminal.write(message)\n",
    "        # Write to file (only if message is not empty)\n",
    "        if message and self.file_handle:\n",
    "            self.file_handle.write(message)\n",
    "            self.file_handle.flush()\n",
    "        \n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        if self.file_handle:\n",
    "            self.file_handle.flush()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close file handle\"\"\"\n",
    "        if self.file_handle:\n",
    "            self.file_handle.close()\n",
    "            self.file_handle = None\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging to timestamped file. Returns log file path.\"\"\"\n",
    "    now = datetime.now()\n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_dir = \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filename: YYYY_MM_DD_HH_MM.txt (e.g., 2025_01_15_14_30.txt)\n",
    "    log_filename = f\"{now.year:04d}_{now.month:02d}_{now.day:02d}_{now.hour:02d}_{now.minute:02d}.txt\"\n",
    "    log_path = os.path.join(log_dir, log_filename)\n",
    "    \n",
    "    # Store original stdout/stderr\n",
    "    original_stdout = sys.stdout\n",
    "    original_stderr = sys.stderr\n",
    "    \n",
    "    # Create TeeOutput instances (they will open the file)\n",
    "    tee_stdout = TeeOutput(original_stdout, log_path)\n",
    "    tee_stderr = TeeOutput(original_stderr, log_path)\n",
    "    \n",
    "    # Redirect stdout and stderr to TeeOutput\n",
    "    sys.stdout = tee_stdout\n",
    "    sys.stderr = tee_stderr\n",
    "    \n",
    "    # Write header (this will go through TeeOutput, so no duplication)\n",
    "    print('='*80)\n",
    "    print(f\"MACO Pipeline Log - Started at {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Log file: {log_path}\")\n",
    "    print('='*80 + '\\n')\n",
    "    \n",
    "    return log_path\n",
    "\n",
    "# Initialize logging\n",
    "LOG_FILE_PATH = setup_logging()\n",
    "print(f\"[LOG] All output will be saved to: {LOG_FILE_PATH}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3932b5c",
   "metadata": {},
   "source": [
    "## 1) LLM client (LangChain Google GenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2906ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def make_llm(model: str, temperature: float):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_retries=0,\n",
    "        # relies on GOOGLE_API_KEY env var\n",
    "    )\n",
    "\n",
    "def call_llm_json(llm, system: str, user: str, retry: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call an LLM with system+user text and parse JSON output robustly.\n",
    "    If schema fails, return {\"__SCHEMA_ERROR__\": raw_text}\n",
    "    \"\"\"\n",
    "    msgs = [(\"system\", system), (\"human\", user)]\n",
    "    out = llm.invoke(msgs)\n",
    "    text = getattr(out, \"content\", \"\") or str(out)\n",
    "    \n",
    "    # Strip fencing if present\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", text, flags=re.S)\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        if retry > 0:\n",
    "            nudged = textwrap.dedent(f\"\"\"Your previous reply was not valid JSON. Reprint ONLY strict JSON, no commentary. Original reply: {text}\"\"\")\n",
    "            out2 = llm.invoke([(\"system\", system), (\"human\", nudged)])\n",
    "            t2 = getattr(out2, \"content\", \"\") or str(out2)\n",
    "            t2 = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", t2.strip(), flags=re.S)\n",
    "            try:\n",
    "                return json.loads(t2)\n",
    "            except Exception:\n",
    "                return {\"__SCHEMA_ERROR__\": t2}\n",
    "        return {\"__SCHEMA_ERROR__\": text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49366ba",
   "metadata": {},
   "source": [
    "## 2) Retrieval (Gensee AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00b68547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "def gensee_ai_retrieve(query: str, max_results: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves context snippets using the Gensee AI Platform API.\n",
    "\n",
    "    Notes:\n",
    "        - Relies on an environment variable `GENSEE_API_KEY` to get the 'Bearer your_token_here'.\n",
    "        - Returns an empty list if the request fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    api_key = os.getenv(\"GENSEE_API_KEY\",)\n",
    "    if not api_key:\n",
    "        print(\"[WARN] Missing GENSEE_API_KEY environment variable — returning empty list.\")\n",
    "        return []\n",
    "\n",
    "    # 2. Prepare the API request\n",
    "    url = 'https://platform.gensee.ai/tool/search'\n",
    "    \n",
    "    # 3. Build the payload matching your API's requirements\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'max_results': max_results\n",
    "    }\n",
    "    \n",
    "    # 4. Build the headers matching your API's requirements\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}' # Dynamically load the key from env\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 5. Send the POST request\n",
    "        response = requests.post(url, json=data, headers=headers, timeout=60)\n",
    "        response.raise_for_status() # Raise an exception for bad statuses (401, 403, 500, etc.)\n",
    "        data = response.json()\n",
    "\n",
    "        # 6. Parse your specific JSON response structure\n",
    "        #    Based on your example, results are in the 'search_response' key\n",
    "        results = data.get(\"search_response\", [])\n",
    "        \n",
    "        contexts = []\n",
    "        for item in results:\n",
    "            # Based on your example, the text snippet is in the 'content' key\n",
    "            snippet = item.get(\"content\") or \"\"\n",
    "            if snippet:\n",
    "                contexts.append(snippet)\n",
    "        \n",
    "        # 7. Ensure a List[str] is returned\n",
    "        return [ctx for ctx in contexts if ctx][:max_results]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[WARN] Gensee AI request failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f2540",
   "metadata": {},
   "source": [
    "## 3) Prompts (Query, Evaluator, Analyst, Editor, Selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15b2aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUERY_SYSTEM = \"\"\"You generate user queries for evaluating ONE article.\n",
    "Produce content-centric queries:\n",
    "- Each query MUST be answerable using the article alone.\n",
    "- Cover varied user intents: {definition, learn/explanation, how-to/guide, compare, list/examples}.\n",
    "- Avoid brand bait, clickbait, or unanswerable questions.\n",
    "Return your entire response in STRICT JSON::\n",
    "{\"queries\":[\n",
    "  {\"intent\":\"definition\",\"q\":\"...\"},\n",
    "  {\"intent\":\"learn\",\"q\":\"...\"},\n",
    "  {\"intent\":\"how-to\",\"q\":\"...\"},\n",
    "  {\"intent\":\"compare\",\"q\":\"...\"},\n",
    "  {\"intent\":\"list\",\"q\":\"...\"}\n",
    "]}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_query_user(doc: str, n_queries: int = N_QUERIES) -> str:\n",
    "    return f\"[ARTICLE]\\n{doc}\\n\\nReturn {n_queries} queries spread across the intents.\"\n",
    "\n",
    "PROMPT_EVAL_SYSTEM = \"\"\"You are an evaluation judge. Given a candidate article and external contexts:\n",
    "1) Answer the user query (RAG style).\n",
    "2) Score the CANDIDATE ARTICLE on SIX dimensions in [0,1] using anchors {0,.17,.33,.5,.67,.83,1}:\n",
    "   - CP (Citation Prominence): clear, prominent citation/attribution of the candidate article in the final answer.\n",
    "   - AA (Attribution Accuracy): statements attributed to the article truly originate from it.\n",
    "   - FA (Faithfulness): answer remains faithful to the article’s meaning (no distortions).\n",
    "   - KC (Key Concepts): article covers essential concepts needed for this query.\n",
    "   - SC (Semantic Contribution): article contributes unique/central meaning vs other contexts.\n",
    "   - AD (Answer Dominance): overall share of answer content deriving from the article vs other contexts.\n",
    "Rules:\n",
    "- Judge ONLY the candidate article’s contribution; do not reward contexts.\n",
    "- If the answer can be formed without the article, penalize SC and AD.\n",
    "- If external contexts are absent or minimal relative to the answer, DO NOT award SC or AD above 0.33 unless you explicitly justify why the article itself supplies the necessary unique content.\n",
    "- If the article is very short/sparse and lacks definitions/examples/comparisons needed by the query, reduce KC and FA accordingly.\n",
    "Return your entire response in STRICT JSON:\n",
    "{\n",
    " \"answer\": \"...\",\n",
    " \"scores\": {\"CP\":0.83,\"AA\":0.67,\"FA\":0.83,\"KC\":0.67,\"SC\":0.50,\"AD\":0.50},\n",
    " \"why\": {\n",
    "   \"CP\":\"...\", \"AA\":\"...\", \"FA\":\"...\", \"KC\":\"...\", \"SC\":\"...\", \"AD\":\"...\"\n",
    " }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_eval_user(query: str, doc: str, contexts: List[str]) -> str:\n",
    "    ctx = \"\\n---\\n\".join(contexts[:MAX_CTX]) if contexts else \"(no external contexts)\"\n",
    "    return f\"[QUERY]\\n{query}\\n\\n[CANDIDATE_ARTICLE]\\n{doc}\\n\\n[CONTEXTS]\\n{ctx}\"\n",
    "\n",
    "PROMPT_ANALYST_SYSTEM = \"\"\"You propose targeted edits to improve the article’s weakest metrics.\n",
    "Inputs: (1) article, (2) per-query scores with brief rationales, (3) aggregate MIS/ISR/MIV.\n",
    "Find the single weakest metric by MIS; break ties by high MIV and low ISR.\n",
    "Propose up to 3 precise edits. For EACH edit include:\n",
    "- target_metric: one of {CP,AA,FA,KC,SC,AD}\n",
    "- reason: ≤2 sentences\n",
    "- location_hint: exact anchor text or section title\n",
    "- operation: one of {\"insert_after\",\"replace_span\",\"append_section\",\"delete_span\",\"merge_sections\"}\n",
    "- patch: exact text to insert/replace (≤180 words)\n",
    "Return your entire response in STRICT JSON:: {\"edits\":[{...}, {...}]}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_analyst_user(doc: str, per_query: List[Dict[str, Any]], agg: Dict[str, Any]) -> str:\n",
    "    return json.dumps({\n",
    "        \"article\": doc,\n",
    "        \"per_query\": per_query,\n",
    "        \"aggregate\": agg\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "PROMPT_EDITOR_SYSTEM = \"\"\"Apply ONE provided edit to the article faithfully. \n",
    "Do NOT rewrite unrelated text. If location_hint not found, place patch in the nearest logical spot.\n",
    "Return the FULL revised article only. No explanations.\n",
    "\"\"\"\n",
    "\n",
    "def prompt_editor_user(doc: str, json_edit: Dict[str, Any]) -> str:\n",
    "    return json.dumps({\"article\": doc, \"edit\": json_edit}, ensure_ascii=False)\n",
    "\n",
    "PROMPT_SELECTOR_SYSTEM = \"\"\"You are a selector comparing multiple article versions evaluated on the SAME query+context corpus.\n",
    "Given MIS, ISR, MIV per version, pick the version that maximizes:\n",
    "score = sum(MIS[m] for m in [CP,AA,FA,KC,SC,AD]) - 0.2 * sum(MIV[m] for m in [CP,AA,FA,KC,SC,AD]).\n",
    "Return your entire response in STRICT JSON:: {\"winner_index\": k, \"reason\":\"≤2 sentences\"}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_selector_user(history_summary: List[Dict[str, Any]]) -> str:\n",
    "    # history_summary: [{\"idx\": i, \"agg\": {...}, \"snippet\": \"...\"}]\n",
    "    return json.dumps({\"candidates\": history_summary}, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adf5e2",
   "metadata": {},
   "source": [
    "## 4) Query generation + frozen corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aff1994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus (build once, then freeze) \n",
    "def generate_queries_from_doc(doc_text: str, n_queries: int = N_QUERIES) -> List[str]:\n",
    "    llm = make_llm(MODEL_ANALYST, temperature=0.3)  # tiny diversity, still on-topic\n",
    "    payload = call_llm_json(llm, PROMPT_QUERY_SYSTEM, prompt_query_user(doc_text, n_queries))\n",
    "    if \"__SCHEMA_ERROR__\" in payload:\n",
    "        # very robust fallback: produce 5 generic but doc-specific queries\n",
    "        base = [\n",
    "            \"Give a concise definition.\",\n",
    "            \"Explain the key benefits.\",\n",
    "            \"Provide a simple example.\",\n",
    "            \"Compare it with an alternative.\",\n",
    "            \"Give a short step-by-step guide.\"\n",
    "        ]\n",
    "        return [f\"{q} (based on the article above)\" for q in base][:n_queries]\n",
    "    qs = [q[\"q\"] for q in payload.get(\"queries\", []) if q.get(\"q\")]\n",
    "    # dedupe, cap\n",
    "    seen, uniq = set(), []\n",
    "    for q in qs:\n",
    "        if q not in seen:\n",
    "            uniq.append(q)\n",
    "            seen.add(q)\n",
    "    \n",
    "    if DEBUG:\n",
    "        log_heading(\"Query Agent: generated queries\")\n",
    "        for i, q in enumerate(uniq[:n_queries]):\n",
    "            print(f\"{i+1}. {q}\")\n",
    "\n",
    "    return uniq[:n_queries]\n",
    "\n",
    "def build_corpus_for_doc(doc_text: str, retriever=gensee_ai_retrieve,\n",
    "                         n_queries=N_QUERIES, max_ctx=MAX_CTX) -> Dict[str, Any]:\n",
    "    queries = generate_queries_from_doc(doc_text, n_queries=n_queries)\n",
    "    pairs = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            ctxs = retriever(q)[:max_ctx]\n",
    "        except Exception as e:\n",
    "            ctxs = []\n",
    "        # keep only queries with at least 2 contexts (so the judge can compare)\n",
    "        cleaned = []\n",
    "        for c in ctxs:\n",
    "            c = re.sub(r\"\\s+\", \" \", c.strip())\n",
    "            if c and c not in cleaned:\n",
    "                cleaned.append(c)\n",
    "        if len(cleaned) >= 2:\n",
    "            pairs.append({\"q\": q, \"ctx\": cleaned})\n",
    "    if DEBUG:\n",
    "        log_heading(\"Retrieval: per-query context counts\")\n",
    "        for p in pairs:\n",
    "            print(f\"- {p['q'][:80]}...  | ctx={len(p['ctx'])}\")\n",
    "        log_heading(\"Retrieved Contexts (Full Content)\")\n",
    "        for i, p in enumerate(pairs):\n",
    "            print(f\"\\n--- Query {i+1}: {p['q']} ---\")\n",
    "            for j, ctx in enumerate(p['ctx']):\n",
    "                print(f\"\\n[Context {j+1}]\")\n",
    "                print(ctx[:500] + (\"...\" if len(ctx) > 500 else \"\"))\n",
    "\n",
    "    # require minimum coverage\n",
    "    if len(pairs) < 2:\n",
    "        raise RuntimeError(f\"Corpus too small ({len(pairs)} with >=2 contexts). \"\n",
    "                           f\"Set GENSEE_API_KEY and retry, or reduce filters.\")\n",
    "    key = hashlib.md5(doc_text.encode()).hexdigest()[:10]\n",
    "    path = f\"corpus_{key}.json\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"queries\": pairs, \"created_at\": time.time()}, f, ensure_ascii=False, indent=2)\n",
    "    return {\"queries\": pairs, \"path\": path}\n",
    "\n",
    "    \n",
    "def load_corpus(path: str) -> Dict[str, Any]:\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a6b0c",
   "metadata": {},
   "source": [
    "## 5) Evaluator (per-query + MIS/ISR/MIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fbdc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def _nearest_anchor(x: float) -> float:\n",
    "    # snap to anchor grid\n",
    "    if x is None: return 0.0\n",
    "    try: x = float(x)\n",
    "    except: return 0.0\n",
    "    return min(ANCHORS, key=lambda a: abs(a - x))\n",
    "\n",
    "def evaluator_score(document: str, query: str, contexts: List[str]) -> Dict[str, Any]:\n",
    "    if DEBUG:\n",
    "        log_heading(f\"Evaluator: Query & Contexts\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"\\nNumber of contexts: {len(contexts)}\")\n",
    "        for i, ctx in enumerate(contexts):\n",
    "            print(f\"\\n[Context {i+1}]\")\n",
    "            print(ctx[:500] + (\"...\" if len(ctx) > 500 else \"\"))\n",
    "    \n",
    "    llm = make_llm(MODEL_EVAL, TEMPERATURE_EVAL)\n",
    "    payload = call_llm_json(llm, PROMPT_EVAL_SYSTEM, prompt_eval_user(query, document, contexts))\n",
    "    if DEBUG:\n",
    "        log_heading(\"Evaluator Output\")\n",
    "        log_json(\"payload\", payload)\n",
    "\n",
    "    answer = payload.get(\"answer\", \"\")\n",
    "    raw_scores = (payload.get(\"scores\") or {})\n",
    "    why = payload.get(\"why\") or {}\n",
    "    # coerce to anchors & fill missing\n",
    "    scores = {m: _nearest_anchor(raw_scores.get(m)) for m in METRICS}\n",
    "    return {\"query\": query, \"scores\": scores, \"why\": why, \"answer\": answer}\n",
    "\n",
    "def aggregate_scores(per_query_scores: List[Dict[str, Any]], tau: float = SUCCESS_TAU) -> Dict[str, Dict[str, float]]:\n",
    "    arr = np.array([[pq[\"scores\"][m] for m in METRICS] for pq in per_query_scores])  # shape Qx6\n",
    "    mis = dict(zip(METRICS, arr.mean(axis=0).round(4).tolist()))\n",
    "    isr = dict(zip(METRICS, (arr >= tau).mean(axis=0).round(4).tolist()))\n",
    "    miv = dict(zip(METRICS, arr.var(axis=0, ddof=0).round(4).tolist()))\n",
    "    return {\"MIS\": mis, \"ISR\": isr, \"MIV\": miv}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a8cd9",
   "metadata": {},
   "source": [
    "## 6) Analyst (edits) + tag detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70980ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst_propose_edits(doc: str, per_query: List[Dict[str, Any]], agg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    llm = make_llm(MODEL_ANALYST, TEMPERATURE_ANALYST)\n",
    "    payload = call_llm_json(llm, PROMPT_ANALYST_SYSTEM, prompt_analyst_user(doc, per_query, agg))\n",
    "    if DEBUG:\n",
    "        log_heading(\"Analyst: proposed edits\")\n",
    "        log_json(\"edits\", payload)\n",
    "\n",
    "    if \"__SCHEMA_ERROR__\" in payload:\n",
    "        # conservative fallback: add benefits sentence (improves SC/KC)\n",
    "        return {\"edits\": [{\n",
    "            \"target_metric\": \"SC\",\n",
    "            \"reason\": \"Add explicit benefits to improve semantic contribution and sufficiency.\",\n",
    "            \"location_hint\": \"After introduction\",\n",
    "            \"operation\": \"insert_after\",\n",
    "            \"patch\": \"Key benefits include clarity, coverage of essential concepts, and concrete examples that distinguish this article from generic sources.\"\n",
    "        }]}\n",
    "    # auto-tag the proposed patches\n",
    "    for e in payload.get(\"edits\", []):\n",
    "        patch = e.get(\"patch\", \"\")\n",
    "        for tag, pat in TAG_PATTERNS:\n",
    "            if re.search(pat, patch, flags=re.I):\n",
    "                e[\"tag\"] = tag\n",
    "                break\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf84a7",
   "metadata": {},
   "source": [
    "## 7) Editor (apply one edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "019c39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_edit_locally(doc: str, edit: Dict[str, Any]) -> str:\n",
    "    \"\"\"Lightweight, deterministic local editor for simple ops before LLM.\"\"\"\n",
    "    op = edit.get(\"operation\")\n",
    "    hint = edit.get(\"location_hint\",\"\")\n",
    "    patch = edit.get(\"patch\",\"\").strip()\n",
    "\n",
    "    if not patch and op != \"delete_span\":\n",
    "        return doc\n",
    "\n",
    "    if op == \"insert_after\":\n",
    "        idx = doc.find(hint) if hint else -1\n",
    "        if idx >= 0:\n",
    "            cut = idx + len(hint)\n",
    "            return doc[:cut] + (\"\\n\" if doc[cut:cut+1] != \"\\n\" else \"\") + patch + \"\\n\" + doc[cut:]\n",
    "        else:\n",
    "            # append near end\n",
    "            return doc.rstrip() + \"\\n\\n\" + patch + \"\\n\"\n",
    "\n",
    "    if op == \"replace_span\":\n",
    "        if hint and hint in doc:\n",
    "            return doc.replace(hint, patch, 1)\n",
    "        return doc  # fallback: no-op\n",
    "\n",
    "    if op == \"append_section\":\n",
    "        return doc.rstrip() + \"\\n\\n\" + patch + \"\\n\"\n",
    "\n",
    "    if op == \"delete_span\":\n",
    "        if hint and hint in doc:\n",
    "            return doc.replace(hint, \"\", 1)\n",
    "        return doc\n",
    "\n",
    "    if op == \"merge_sections\":\n",
    "        # naive: remove duplicate consecutive blank lines (simplify structure)\n",
    "        merged = re.sub(r\"\\n{3,}\", \"\\n\\n\", doc)\n",
    "        return merged\n",
    "\n",
    "    return doc\n",
    "\n",
    "def editor_apply_edit(doc: str, chosen_edit: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    First try a deterministic local application; if the hint isn't found or\n",
    "    the operation needs rewriting, fall back to the LLM editor.\n",
    "    \"\"\"\n",
    "    # Try local\n",
    "    new_doc = _apply_edit_locally(doc, chosen_edit)\n",
    "    if new_doc != doc or chosen_edit.get(\"operation\") in (\"append_section\",\"merge_sections\",\"delete_span\"):\n",
    "        return new_doc\n",
    "\n",
    "    # Fallback to LLM editor for tougher cases\n",
    "    llm = make_llm(MODEL_EDITOR, TEMPERATURE_EDITOR)\n",
    "    out = llm.invoke([(\"system\", PROMPT_EDITOR_SYSTEM),\n",
    "                      (\"human\", prompt_editor_user(doc, chosen_edit))])\n",
    "    text = getattr(out, \"content\", \"\") or str(out)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817b66e",
   "metadata": {},
   "source": [
    "## 8) Optimize loop + hybrid selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bc4d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _history_summary_for_selector(history: List[Tuple[str, List[Dict[str,Any]], Dict[str,Any]]]) -> List[Dict[str, Any]]:\n",
    "    summ = []\n",
    "    for i, (doc, perq, agg) in enumerate(history):\n",
    "        # a short snippet for context\n",
    "        snippet = (doc[:220] + \"…\") if len(doc) > 220 else doc\n",
    "        summ.append({\"idx\": i, \"agg\": agg, \"snippet\": snippet})\n",
    "    return summ\n",
    "\n",
    "def score_scalar(agg: Dict[str, Dict[str, float]], lam: float = 0.2) -> float:\n",
    "    s = sum(agg[\"MIS\"][m] for m in METRICS) - lam * sum(agg[\"MIV\"][m] for m in METRICS)\n",
    "    return round(float(s), 4)\n",
    "\n",
    "def select_best_version(history: List[Tuple[str, List[Dict[str,Any]], Dict[str,Any]]]) -> Dict[str, Any]:\n",
    "    # 1) rule-based ranking\n",
    "    with_scores = [(i, score_scalar(agg)) for i, (_, _, agg) in enumerate(history)]\n",
    "    with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top = [i for i,_ in with_scores[:3]]\n",
    "\n",
    "    # 2) LLM selector tie-breaker among top-3 (optional; safer)\n",
    "    llm = make_llm(MODEL_EVAL, 0.0)\n",
    "    summary = _history_summary_for_selector([history[i] for i in top])\n",
    "    payload = call_llm_json(llm, PROMPT_SELECTOR_SYSTEM, prompt_selector_user(summary))\n",
    "    if \"__SCHEMA_ERROR__\" in payload:\n",
    "        # fallback to best scalar\n",
    "        best_idx = top[0]\n",
    "    else:\n",
    "        k = payload.get(\"winner_index\", 0)\n",
    "        best_idx = top[min(max(int(k), 0), len(top)-1)]\n",
    "\n",
    "    doc, perq, agg = history[best_idx]\n",
    "    return {\"index\": best_idx, \"doc\": doc, \"agg\": agg, \"score_scalar\": score_scalar(agg)}\n",
    "\n",
    "def optimize_doc(doc_text: str, corpus: Dict[str, Any], n_iters: int = N_ITERS):\n",
    "    history = []\n",
    "    D = doc_text\n",
    "    for t in range(n_iters):\n",
    "        # Evaluate on the frozen corpus\n",
    "        per_query_scores = []\n",
    "        for item in corpus[\"queries\"]:\n",
    "            scores = evaluator_score(D, item[\"q\"], item[\"ctx\"])\n",
    "            per_query_scores.append(scores)\n",
    "        agg = aggregate_scores(per_query_scores, tau=SUCCESS_TAU)\n",
    "        history.append((D, per_query_scores, agg))\n",
    "\n",
    "        # Analyze & choose an edit\n",
    "        plan = analyst_propose_edits(D, per_query_scores, agg)\n",
    "        edits = plan.get(\"edits\", [])\n",
    "        if not edits:\n",
    "            # nothing to do -> early stop\n",
    "            break\n",
    "        # Choose the edit most aligned with weakest metric (by MIS)\n",
    "        mis = agg[\"MIS\"]\n",
    "        weakest = sorted(METRICS, key=lambda m: mis[m])[0]\n",
    "        chosen = next((e for e in edits if e.get(\"target_metric\")==weakest), edits[0])\n",
    "        if DEBUG:\n",
    "            log_heading(f\"ITER {t} — Chosen edit\")\n",
    "            log_json(\"chosen_edit\", chosen)\n",
    "\n",
    "        # Apply\n",
    "        D = editor_apply_edit(D, chosen)\n",
    "\n",
    "        if DEBUG:\n",
    "            log_heading(f\"ITER {t} — Editor Output (New Document)\")\n",
    "            print(D) \n",
    "            print(\"=\"*80)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485022a",
   "metadata": {},
   "source": [
    "## 9) Run end-to-end (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c2fdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DOC = \"\"\"\\\n",
    "API (Application Programming Interface) is a set of rules and definitions that\n",
    "allows applications to communicate with each other. Developers use APIs to access\n",
    "data or functionality from external services without knowing their internal implementations.\n",
    "\"\"\"\n",
    "\n",
    "# 1) Build (or load) frozen corpus for this document\n",
    "corpus = build_corpus_for_doc(SOURCE_DOC)  # returns {\"queries\":[...], \"path\": ...}\n",
    "print(f\"Frozen corpus saved to: {corpus['path']} with {len(corpus['queries'])} queries.\")\n",
    "\n",
    "# 2) Iterate\n",
    "hist = optimize_doc(SOURCE_DOC, corpus, n_iters=N_ITERS)\n",
    "\n",
    "# 3) Select best version\n",
    "best = select_best_version(hist)\n",
    "\n",
    "# 4) Report\n",
    "print(\"\\n=== Iteration summary (MIS per iter) ===\")\n",
    "for i, (_, _, agg) in enumerate(hist):\n",
    "    mis_line = \" \".join([f\"{m}:{agg['MIS'][m]:.2f}\" for m in METRICS])\n",
    "    print(f\"iter {i:02d} | {mis_line} | scalar={score_scalar(agg):.3f}\")\n",
    "\n",
    "print(\"\\n=== Winner ===\")\n",
    "print(f\"Iteration: {best['index']}, scalar={best['score_scalar']:.3f}\")\n",
    "print(best[\"agg\"])\n",
    "print(\"\\n=== Selected Document ===\\n\")\n",
    "print(best[\"doc\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
